# -*- coding: utf-8 -*-
"""myfinal multivariate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hMjyRE3NlWwVdKpMV2DpvKNt8NwkGGiH

## Regression Task
### Libraries and csv data imports
"""

#importing pandas and numpy
import pandas as pd
import numpy as np

#importing csv file cars.csv
df = pd.read_csv('cars.csv', header=None)  #import dataset as df

#printing top 5 rows of data
df.head()

"""### Data Preprocessing"""

#concatinating bias terms
df = pd.concat([pd.Series(1, index=df.index, name='00'), df], axis=1)   
df.head()

"""### Splitting explanatory and output variables"""

x = df.drop(columns=10)  #splitting columns and taking in explanatory variables as x
x.head() #Showing top 5 rows of x

y = df.iloc[:, 11]  #splitting column and taking in the output variable as y
y.head() #Showing top 5 rows of y

"""#### Normalization"""

#normalization to get each column into same scale
for i in range(1, len(x.columns)):  
    x[i-1] = x[i-1]/np.max(x[i-1]) 
x.head() #Displays data after normalization

theta = np.array([0]*len(x.columns))  #theta initiazed as 0
theta

m = len(df)  #assign number of training data to m
m #display m

def hypothesis(theta, x):  #defining hypothesis function
    return theta*x

def cost(x, y, theta):  #defining cost function
    y1 = hypothesis(theta, x)
    y1=np.sum(y1, axis=1)
    return sum(np.sqrt((y1-y)**2))/(2*100)

#defining gradient descent taking x,y,theta,alpha (learning rate) and iteration
def gradientDescent(x, y, theta, alpha, i):
    J = []  #cost function 
    k = 0
    while k < i:        
        y1 = hypothesis(theta, x)
        y1 = np.sum(y1, axis=1)
        for c in range(0, len(x.columns)):
            theta[c] = theta[c] - alpha*(sum((y1-y)*x.iloc[:,c])/len(x))
        j = cost(x, y, theta)
        J.append(j)
        k += 1
    return J, j, theta

#getting the final cost from gradient descent function with theta optimization
J, j, theta = gradientDescent(x, y, theta, 0.2, 10000)

#output prediction with the help of theta optimization
y_cap = hypothesis(theta, x)
y_cap = np.sum(y_cap, axis=1)

# Commented out IPython magic to ensure Python compatibility.
#ploting the original output value and predicted output value
# %matplotlib inline
import matplotlib.pyplot as plt #importing matplotlib to plot into scatterplot
plt.figure() 
plt.scatter(x=list(range(0, 100)),y= y, color='blue')         
plt.scatter(x=list(range(0, 100)), y=y_cap, color='red')
plt.show()

#ploting the cost of each iteration
plt.figure()
plt.scatter(x=list(range(0, 10000)),color = "blue", y=J)
plt.show()

#importing r2_score from sklearn to calculate accuracy of the model
from sklearn.metrics import r2_score

#printing r2 score
print(r2_score(y,y_cap))

#implementing r2_score to calculate accuracy
Accuracy = r2_score(y,y_cap)

#formatting accuracy score and printing it
# print("The Accuracy of the model is: {:.2f}".format(Accuracy))
print("The accuracy of the model is:", int(Accuracy*100), '%')